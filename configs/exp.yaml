# exp.yaml
project:
  name: narrativity-interference
  seed: 42

paths:
  # ----- inputs -----
  elements: data/elements/common_elements.json
  base_data: data/stories/base_data.json
  distractor: data/distractors/distractor.json
  qa: data/qa/qa.json

  # ----- outputs root -----
  outputs: outputs

# =========================
# 1) Dataset generation
# =========================
dataset:
  num_chapters: 100
  events_per_chapter: 10

  # story generation (OpenAI) 
  openai_model: gpt-3.5-turbo
  max_retries: 2
  style:
    name: Shakespearean
    description: >-
      emulating the poetic language, dramatic flair, and rhythmic cadence wordistic of Shakespeare's plays,
      including the use of iambic pentameter, archaic expressions, and heightened emotion

  distractor:
    # C = ratio * (#words of high story)
    ratio: 1.0
    mi_phrase: "Humpty Dumpty" 

# =========================
# 2) Inference
# =========================
inference:
  conditions: [NI, MI, RI, UI]
  levels: [h, l]

  generation:
    max_new_tokens: 80
    temperature: 0.0
    top_p: 1.0

  save:
    responses_dir: outputs/responses
    metrics_dir: outputs/metrics

models:
  llama:
    name: meta-llama/Llama-2-13b-chat-hf
    dtype: float16
    device: cuda
    max_memory:
      cuda0: 24GiB
      cpu: 64GiB
    offload_dir: offload_llama2

  qwen:
    name: Qwen/Qwen2.5-14B-Instruct
    dtype: float16
    device: cuda
    max_memory:
      cuda0: 24GiB
      cpu: 64GiB
    offload_dir: offload_qwen

# =========================
# 3) Evaluation / Visualization
# =========================
eval:
  heatmap:
    topics: [location, temporal, entity, content]
    distractor_labels: [NI, MI, RI, UI]
    topic_labels_short: [s, t, ent, c]
    output_dir: outputs/figures/heatmap
    vmin_vmax:
      llama: [0.4, 0.6]
      qwen: [0.7, 1.0]

attention:
  batch_size: 10
  output_dir: outputs/attention
  labels: [temporal, location, entity, content, other]
  regions: [text, distractor, question_other]
